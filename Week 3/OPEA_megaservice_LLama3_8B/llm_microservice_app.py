# llm_microservice_app.py (Final version with all previous fixes)

import os
import time
import requests
import json
from comps import MicroService, ServiceType, ServiceRoleType
from comps.cores.proto.api_protocol import ChatCompletionRequest, ChatCompletionResponse, ChatCompletionResponseChoice, ChatMessage
from pydantic import BaseModel
from typing import Dict, Any

# --- Environment Variables (for this service) ---
LLM_SERVICE_HOST_IP = os.getenv("LLM_SERVICE_HOST_IP", "0.0.0.0")
LLM_SERVICE_PORT = os.getenv("LLM_SERVICE_PORT", 9000)

# --- TGI Server Configuration ---
TGI_SERVER_URL = os.getenv("TGI_SERVER_URL", "http://127.0.0.1:8080")
TGI_TIMEOUT = int(os.getenv("TGI_TIMEOUT", 600)) # Timeout for TGI requests in seconds

# --- Define Usage class locally since it's not importable from comps ---
class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class LLMMicroservice:
    def __init__(self, host="0.0.0.0", port=9000):
        self.host = host
        self.port = port
        self.endpoint = "/v1/chat/completions" # This is the endpoint the Megaservice calls

        print(f"LLM Microservice (TGI Client) initialized. TGI Server URL: {TGI_SERVER_URL}")

    async def handle_request(self, request: ChatCompletionRequest):
        print(f"LLM Microservice received request from Megaservice: {request.messages}")

        user_message_content = ""
        # FIX: Reverting to dictionary access for message content
        for message_dict in request.messages: # Assuming message_dict is a dict
            if message_dict.get("role") == "user":
                user_message_content = message_dict.get("content", "")
                break

        if not user_message_content:
            print("No user message found in the request.")
            return ChatCompletionResponse(
                id="error-no-user-message-" + str(int(time.time())),
                choices=[ChatCompletionResponseChoice(
                    index=0,
                    message=ChatMessage(role="assistant", content="Error: No user message provided."),
                    finish_reason="stop"
                )],
                created=int(time.time()),
                model=request.model if request.model else "default-tgi-model",
                object="chat.completion",
                usage=Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0).model_dump()
            )

        # Prepare payload for TGI server
        # For a basic Llama 3 instruct model, often just sending the user content is enough.
        # For more complex chat interactions, you'd format the prompt using Llama 3's chat template.
        tgi_payload = {
            "inputs": user_message_content, # Sending the user's message directly to TGI
            "parameters": {
                "max_new_tokens": 500,
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.95,
                # "stop": ["<|eot_id|>", "<|end_of_text|>"] # Uncomment for Llama 3 specific stop tokens if needed
            }
        }

        try:
            # Send POST request to the TGI server
            print(f"Sending request to TGI: {TGI_SERVER_URL}/generate with payload: {tgi_payload}")
            response = requests.post(f"{TGI_SERVER_URL}/generate", json=tgi_payload, timeout=TGI_TIMEOUT)
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

            tgi_response_data = response.json()
            print(f"Received response from TGI: {tgi_response_data}")

            generated_text = "No content generated by LLM."
            prompt_tokens = 0
            completion_tokens = 0

            # Parse TGI response
            # TGI's /generate endpoint returns a list of results.
            if isinstance(tgi_response_data, list) and tgi_response_data:
                first_result = tgi_response_data[0]
                if 'generated_text' in first_result:
                    generated_text = first_result['generated_text']
                if 'details' in first_result:
                    details = first_result['details']
                    if 'tokens' in details:
                        completion_tokens = len(details['tokens'])
                    if 'prefill' in details:
                         prompt_tokens = len(details['prefill']) # Prefill contains input tokens

            # Fallback for prompt_tokens if TGI details don't provide them
            if prompt_tokens == 0:
                prompt_tokens = len(user_message_content.split()) # Simple word count as approximation

            choice = ChatCompletionResponseChoice(
                index=0,
                message=ChatMessage(role="assistant", content=generated_text),
                finish_reason="stop" # Assuming generation finishes successfully
            )

            response_usage = Usage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens
            )

            return ChatCompletionResponse(
                id="chatcmpl-" + str(int(time.time())),
                choices=[choice],
                created=int(time.time()),
                model=request.model if request.model else "your-tgi-model",
                object="chat.completion",
                usage=response_usage.model_dump()
            )

        except requests.exceptions.RequestException as e:
            print(f"Error communicating with TGI server: {e}")
            error_message = f"LLM Microservice Error: Could not connect to TGI server or TGI returned an error. Is TGI running at {TGI_SERVER_URL}? Error: {e}"
            choice = ChatCompletionResponseChoice(
                index=0,
                message=ChatMessage(role="assistant", content=error_message),
                finish_reason="stop"
            )
            return ChatCompletionResponse(
                id="error-tgi-comm-" + str(int(time.time())),
                choices=[choice],
                created=int(time.time()),
                model=request.model if request.model else "your-tgi-model",
                object="chat.completion",
                usage=Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0).model_dump()
            )
        except Exception as e:
            print(f"An unexpected error occurred in LLM Microservice: {e}")
            choice = ChatCompletionResponseChoice(
                index=0,
                message=ChatMessage(role="assistant", content=f"LLM Microservice unexpected error: {e}"),
                finish_reason="stop"
            )
            return ChatCompletionResponse(
                id="error-llm-unexpected-" + str(int(time.time())),
                choices=[choice],
                created=int(time.time()),
                model=request.model if request.model else "your-tgi-model",
                object="chat.completion",
                usage=Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0).model_dump()
            )

    def start(self):
        self.service = MicroService(
            self.__class__.__name__,
            service_role=ServiceRoleType.MEGASERVICE, # FINAL FIX: Changed to MEGASERVICE as the error suggested
            host=self.host,
            port=self.port,
            endpoint=self.endpoint,
            input_datatype=ChatCompletionRequest,
            output_datatype=ChatCompletionResponse,
        )

        self.service.add_route(self.endpoint, self.handle_request, methods=["POST"])

        print(f"Starting LLM Microservice (TGI Client) on {self.host}:{self.port}")
        self.service.start()

if __name__ == "__main__":
    llm_service = LLMMicroservice(host=LLM_SERVICE_HOST_IP, port=LLM_SERVICE_PORT)
    llm_service.start()